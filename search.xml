<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>machine learning record day1</title>
      <link href="/2022/10/13/machine-learning-record-day1/"/>
      <url>/2022/10/13/machine-learning-record-day1/</url>
      
        <content type="html"><![CDATA[<h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><blockquote><p>基本概念: 分类与回归<br><br>分类(classfication):预测类别标签<br><br>回归(regression):预测一个连续值<br><strong>机器学习都可通过学习曲线来记录与分析模型性能</strong></p></blockquote><h2 id="1-k近邻"><a href="#1-k近邻" class="headerlink" title="1.k近邻"></a>1.k近邻<br></h2><ul><li><p><em>k</em>近邻分类<br></p><blockquote><p><font color =DarkGreen>k-NN算法是最简单的机器学习算法</font><br><br><font color =DarkGreen>k-NN算法是惰性学习算法 ，依靠记忆训练过的数据集来完成任务</font><br><br>  总结为以下步骤：<br>  1.选择<em>k</em>个数和一个距离度量<br>  2.找到要分类的<em>k</em>-近邻<br>  3.以多票数机制确定分类标签<br>  </p></blockquote><p>  单一最近邻与多个最近邻  采用多个最近邻进行判断分类时，<font color = red>将出现次数多的类别（k近邻中占多数的类别）作为预测结果 </font> 效果图如下所示[左边为单一近邻 右边为3近邻]<br>  <img src="/:/207d984e14ed4c67a80c1d1e0920b380" alt="1663854523999"><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 紧邻模型对于forge()模型预测结果</span></span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span>  train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">X,y= mglearn.datasets.make_forge() <span class="comment">#调用force数据集</span></span><br><span class="line">X_train,X_test,Y_train,y_test = train_test_split(X,y,random_state=<span class="number">0</span>)</span><br><span class="line">clf = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)  <span class="comment">#创建分类器对象</span></span><br><span class="line">clf.fit(X,y) <span class="comment">#用分类器对象训练数据模型</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test set predictions: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(clf.predict(X_test))) <span class="comment">#调用predict方法预测</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test set accuracy:&#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(clf.score(X_test,y_test)))  <span class="comment">#对测试数据和测试标签调用 score方法 ，查看模型精确度</span></span><br><span class="line"></span><br><span class="line">输出结果</span><br><span class="line">Test <span class="built_in">set</span> predictions: [<span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">Test <span class="built_in">set</span> accuracy:<span class="number">0.86</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>分析KNeighborsClassifier  </p><ul><li>更少的邻居对应更复杂的模型但是并非更多的邻居训练精度更好</li></ul></li><li><p>k近邻回归<br></p><blockquote><p><em>k</em>近邻算法也可用于回归，用于回归的<em>k</em>近邻算法使用scikit-learn中的KNeighborsRegressor实现 用法与KNeighborsClassifiter相似</p></blockquote></li><li><p>分析KNeighborsRegressor<br></p></li></ul><h2 id="2-线性模型"><a href="#2-线性模型" class="headerlink" title="2.线性模型"></a>2.线性模型<br></h2><blockquote><p>线性模型中，score方法表示 <em>R<sup>2</sup></em></p></blockquote><ul><li><p>用于回归的线性模型</p><ul><li>线性回归（最小二乘法）<font color = darkgreen>LinearRegression</font>方法<br><code>coef_</code>表示权重系数 ，相当于斜率，而截距保存在<code>intercept_</code>属性中，<code>LinearRegression</code> 方法基本使用方法如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span>  LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span>  train_test_split</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X,y = mglearn.datasets.load_extended_boston() <span class="comment">#调用Boston数据集</span></span><br><span class="line"></span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=<span class="number">0</span>) <span class="comment">#对数据进行划分</span></span><br><span class="line">lr = LinearRegression(alpha = <span class="number">1</span>).fit(X_train,y_train)  <span class="comment">#使用线性回归进行训练数据 alpha参数默认是1  alpha更大 模型约束性更强 alpha小 泛化性强</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>岭回归 <font color = darkgreen>ridge regression</font> <em># 调用方法与线性回归基本一致</em></p><blockquote><p>标准线性回归常用的替代方法之一。 岭回归是一种用于回归的线性模型，其拟合公式与最小二乘法相同，但区别在于 岭回归对于系数<em>w</em>的拟合  ，不仅要求在训练数据上得到较好的结果，还需要拟合附加约束，这个叫做 <font color = red><strong>正则化</strong></font>(岭回归使用的是<font color = red>L2正则化</font>)</p></blockquote><ul><li>lasso <font color = red>(L1正则化)</font> <blockquote><p>使用lasso时省略某些模型特征来训练  ，从而突出某些特征  <code>Lasso()</code>方法两个重要参数<code>alpha</code> 和<code>max_iter</code> max_iter表示迭代最大次数  减小<code>alpha</code>[<font color = red>即为提高模型复杂度</font>]同时要增大迭代次数,从而保证训练次数</p></blockquote></li></ul></li><li><p>用于分类的线性模型</p></li></ul><h2 id="3-朴素贝叶斯分类器"><a href="#3-朴素贝叶斯分类器" class="headerlink" title="3.朴素贝叶斯分类器"></a>3.朴素贝叶斯分类器<br></h2><h2 id="4-决策树"><a href="#4-决策树" class="headerlink" title="4.决策树"></a>4.决策树<br></h2><h2 id="5-神经网络-深度学习"><a href="#5-神经网络-深度学习" class="headerlink" title="5.神经网络(深度学习)"></a>5.神经网络(深度学习)<br></h2>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
